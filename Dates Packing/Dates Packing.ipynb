{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7226ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR      = \"/local/scratch/group/guldigroup/climate_change/wiki_history_rosie/date_tagging_pipeline/tagged_output_2025-09-20_deduped\"\n",
    "SUBFOLDERS    = [\"historical_objects_tagged\", \"history_of_ideologies_tagged\", \"history_of_sports_tagged\"]\n",
    "\n",
    "FILE_GLOB     = \"**/*.csv\"\n",
    "FILENAME_COL  = \"filename\"\n",
    "TAG_COL       = \"date_tagged\"\n",
    "ENCODINGS     = [\"utf-8\", \"utf-8-sig\", \"latin1\"]\n",
    "\n",
    "JOIN_SEP      = \", \"\n",
    "SAFE_CELL_LIMIT  = 30000\n",
    "TARGET_LAST_MIN_RATIO = 0.15\n",
    "MIN_DATES_IN_LAST     = 2\n",
    "\n",
    "import re, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    csv.field_size_limit(2**31-1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DATE_TAG_RE = re.compile(r\"(?is)<\\s*date\\s*>(.*?)<\\s*/\\s*date\\s*>\")\n",
    "TXT_END_RE  = re.compile(r\"\\.txt\\s*$\", re.IGNORECASE)\n",
    "\n",
    "DYNASTY_YEAR_MAP = {\n",
    "    \"han dynasty\": 202, \"qin dynasty\": -221, \"tang dynasty\": 618,\n",
    "    \"song dynasty\": 960, \"yuan dynasty\": 1271, \"ming dynasty\": 1368, \"qing dynasty\": 1644,\n",
    "    \"tokugawa period\": 1603, \"edo period\": 1603, \"meiji era\": 1868, \"taisho era\": 1912,\n",
    "    \"showa era\": 1926, \"heisei era\": 1989, \"reiwa era\": 2019,\n",
    "    \"bronze age\": -1500, \"iron age\": -800, \"classical antiquity\": -500,\n",
    "    \"hellenistic period\": -323, \"roman empire\": 27, \"middle ages\": 1100,\n",
    "    \"renaissance\": 1500, \"enlightenment\": 1700, \"victorian era\": 1837,\n",
    "    \"ottoman empire\": 1453,\n",
    "}\n",
    "\n",
    "ORDINAL_WORDS = {\n",
    "    \"first\":1,\"second\":2,\"third\":3,\"fourth\":4,\"fifth\":5,\"sixth\":6,\"seventh\":7,\n",
    "    \"eighth\":8,\"ninth\":9,\"tenth\":10,\"eleventh\":11,\"twelfth\":12,\"thirteenth\":13,\n",
    "    \"fourteenth\":14,\"fifteenth\":15,\"sixteenth\":16,\"seventeenth\":17,\"eighteenth\":18,\n",
    "    \"nineteenth\":19,\"twentieth\":20,\"twenty-first\":21,\"twenty second\":22,\n",
    "    \"twenty third\":23,\"twenty fourth\":24,\"twenty fifth\":25,\"twenty sixth\":26,\n",
    "    \"twenty seventh\":27,\"twenty eighth\":28,\"twenty ninth\":29,\"thirtieth\":30\n",
    "}\n",
    "\n",
    "def parse_century_number(token: str):\n",
    "    token = token.lower().strip()\n",
    "    m = re.match(r\"^(\\d{1,2})(st|nd|rd|th)?$\", token)\n",
    "    if m: return int(m.group(1))\n",
    "    return ORDINAL_WORDS.get(token)\n",
    "\n",
    "def map_century_to_year(n: int, era: str = \"\", qualifier: str = \"\") -> int:\n",
    "    off = 0 if \"early\" in qualifier else (50 if \"mid\" in qualifier else (75 if \"late\" in qualifier else 0))\n",
    "    era = (era or \"\").upper()\n",
    "    return (-(n*100) if era in (\"BC\",\"BCE\") else (n-1)*100) + off\n",
    "\n",
    "CENT_NUM_RE  = re.compile(r\"(?:(early|mid|late)\\s+)?(\\d{1,2})(?:st|nd|rd|th)?\\s+centur(?:y|ies)(?:\\s*(BC|BCE|AD|CE))?\", re.IGNORECASE)\n",
    "CENT_WORD_RE = re.compile(r\"(?:(early|mid|late)\\s+)?([A-Za-z\\- ]+)\\s+centur(?:y|ies)(?:\\s*(BC|BCE|AD|CE))?\", re.IGNORECASE)\n",
    "DECADE_RE    = re.compile(r\"(?:(early|mid|late)\\s+)?(?:(\\d{3,4})|'\\s?(\\d{2}))0s\\s*(BC|BCE)?\", re.IGNORECASE)\n",
    "YEAR_BC_RE   = re.compile(r\"\\b(\\d{1,4})\\s*(BC|BCE)\\b\", re.IGNORECASE)\n",
    "YEAR_AD_RE   = re.compile(r\"\\b(\\d{1,4})(?:\\s*(AD|CE))?\\b\", re.IGNORECASE)\n",
    "\n",
    "def extract_extra_years(s: str) -> List[str]:\n",
    "    years = []\n",
    "    if not s: \n",
    "        return years\n",
    "\n",
    "    for k,v in DYNASTY_YEAR_MAP.items():\n",
    "        if k in s.lower():\n",
    "            years.append(str(v))\n",
    "\n",
    "    for q,n_str,era in CENT_NUM_RE.findall(s):\n",
    "        years.append(str(map_century_to_year(int(n_str), era, q)))\n",
    "\n",
    "    for q,word,era in CENT_WORD_RE.findall(s):\n",
    "        n = parse_century_number(word.strip().lower())\n",
    "        if n:\n",
    "            years.append(str(map_century_to_year(n, era, q)))\n",
    "\n",
    "    for q, full, short, era in DECADE_RE.findall(s):\n",
    "        year = int(full) if full else (1900+int(short) if int(short)<30 else 1800+int(short))\n",
    "        if era in (\"BC\",\"BCE\"): \n",
    "            year = -year\n",
    "        if q.lower()==\"early\": year += 2\n",
    "        elif q.lower()==\"mid\": year += 5\n",
    "        elif q.lower()==\"late\": year += 8\n",
    "        years.append(str(year))\n",
    "\n",
    "    for y, era in YEAR_BC_RE.findall(s):\n",
    "        years.append(str(-int(y)))\n",
    "\n",
    "    for y, era in YEAR_AD_RE.findall(s):\n",
    "        if era in (\"AD\",\"CE\"):\n",
    "            years.append(str(int(y)))\n",
    "        elif not era:\n",
    "            years.append(str(int(y)))\n",
    "\n",
    "    return years\n",
    "\n",
    "def extract_dates(cell: Optional[str]) -> List[str]:\n",
    "    if not isinstance(cell, str) or not cell:\n",
    "        return []\n",
    "    raw = [m.strip() for m in DATE_TAG_RE.findall(cell)]\n",
    "    out = []\n",
    "    for r in raw:\n",
    "        out.append(r)                      \n",
    "        out.extend(extract_extra_years(r)) \n",
    "    return out\n",
    "\n",
    "def read_csv_any(path: Path, encodings: List[str], cols: list) -> pd.DataFrame:\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, dtype=str, encoding=enc, usecols=lambda c: c in cols)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                return pd.read_csv(path, dtype=str, encoding=enc)\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    print(f\"[SKIP] Could not read {path} ({last_err})\")\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def is_valid_txt_filename(s: Optional[str]) -> bool:\n",
    "    return isinstance(s, str) and bool(TXT_END_RE.search(s.strip()))\n",
    "\n",
    "def pack_dates_into_cells(dates: List[str],\n",
    "                          sep: str = \", \",\n",
    "                          max_len: int = SAFE_CELL_LIMIT,\n",
    "                          target_last_min_ratio: float = TARGET_LAST_MIN_RATIO,\n",
    "                          min_last_items: int = MIN_DATES_IN_LAST) -> List[str]:\n",
    "    if not dates:\n",
    "        return [\"\"]\n",
    "    parts: List[List[str]] = [[]]\n",
    "    cur_len = 0\n",
    "    def piece_len(item: str, is_first: bool) -> int:\n",
    "        return len(item) if is_first else len(sep) + len(item)\n",
    "    for d in dates:\n",
    "        is_first = (len(parts[-1]) == 0)\n",
    "        add_len = piece_len(d, is_first)\n",
    "        if cur_len + add_len <= max_len:\n",
    "            parts[-1].append(d)\n",
    "            cur_len += add_len\n",
    "        else:\n",
    "            parts.append([d])\n",
    "            cur_len = len(d)\n",
    "    if len(parts) == 1:\n",
    "        return [sep.join(parts[0])]\n",
    "    target_min_len = int(max_len * target_last_min_ratio)\n",
    "    last = parts[-1]; prev = parts[-2]\n",
    "    def joined_length(items: List[str]) -> int:\n",
    "        if not items: return 0\n",
    "        total = len(items[0])\n",
    "        for it in items[1:]:\n",
    "            total += len(sep) + len(it)\n",
    "        return total\n",
    "    if (joined_length(last) < target_min_len) or (len(last) < min_last_items):\n",
    "        while prev and ((joined_length(last) < target_min_len) or (len(last) < min_last_items)):\n",
    "            candidate = prev[-1]\n",
    "            cand_add = piece_len(candidate, is_first=(len(last) == 0))\n",
    "            if joined_length(last) + cand_add <= max_len:\n",
    "                last.insert(0, candidate)\n",
    "                prev.pop()\n",
    "            else:\n",
    "                break\n",
    "        parts = [p for p in parts if p]\n",
    "    return [sep.join(p) for p in parts]\n",
    "\n",
    "for sub in SUBFOLDERS:\n",
    "    root = Path(BASE_DIR) / sub\n",
    "    files = sorted(root.glob(FILE_GLOB))\n",
    "    print(f\"\\n[INFO] Folder={sub} 发现 {len(files)} CSV 文件\")\n",
    "\n",
    "    fn_to_dates: Dict[str, List[str]] = {}\n",
    "\n",
    "    for f in files:\n",
    "        df = read_csv_any(f, ENCODINGS, [FILENAME_COL, TAG_COL])\n",
    "        if df.empty: continue\n",
    "        if FILENAME_COL not in df.columns or TAG_COL not in df.columns: continue\n",
    "        df = df[df[FILENAME_COL].apply(is_valid_txt_filename)]\n",
    "        for _, r in df.iterrows():\n",
    "            fn = r[FILENAME_COL].strip()\n",
    "            ds = extract_dates(r.get(TAG_COL, \"\"))\n",
    "            if not ds: continue\n",
    "            fn_to_dates.setdefault(fn, []).extend(ds)\n",
    "\n",
    "    packed_records = []\n",
    "    max_parts = 0\n",
    "    for fn, dl in fn_to_dates.items():\n",
    "        parts = pack_dates_into_cells(dl, sep=JOIN_SEP, max_len=SAFE_CELL_LIMIT)\n",
    "        max_parts = max(max_parts, len(parts))\n",
    "        packed_records.append((fn, parts))\n",
    "\n",
    "    columns = [\"filename\"] + [f\"dates_part{i}\" for i in range(1, max_parts+1)]\n",
    "    rows = []\n",
    "    for fn, parts in packed_records:\n",
    "        row = {\"filename\": fn}\n",
    "        for i in range(max_parts):\n",
    "            row[f\"dates_part{i+1}\"] = parts[i] if i < len(parts) else \"\"\n",
    "        rows.append(row)\n",
    "    out_df = pd.DataFrame(rows, columns=columns).sort_values(\"filename\")\n",
    "\n",
    "    desktop = Path.home() / \"Desktop\"\n",
    "    packed_file = desktop / f\"filename_dates_packed_{sub}.csv\"\n",
    "    out_df.to_csv(packed_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "    print(f\"[OK] Packed → {packed_file}\")\n",
    "\n",
    "    parsed_rows = []\n",
    "    for fn, dl in fn_to_dates.items():\n",
    "        years = []\n",
    "        for d in dl:\n",
    "            years.extend(extract_extra_years(d))\n",
    "        parsed_rows.append({\"filename\": fn, \"parsed_years\": \", \".join(years)})\n",
    "    parsed_df = pd.DataFrame(parsed_rows).sort_values(\"filename\")\n",
    "    parsed_file = desktop / f\"filename_dates_parsed_{sub}.csv\"\n",
    "    parsed_df.to_csv(parsed_file, index=False, encoding=\"utf-8\", quoting=csv.QUOTE_ALL)\n",
    "    print(f\"[OK] Parsed → {parsed_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
